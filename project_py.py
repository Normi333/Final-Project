# -*- coding: utf-8 -*-
"""Project.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A0BpBbJ6_e13QWRjfsGCSzFSjL2ECXdm
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, utils
from torchvision.models import vgg19
from pytorch_msssim import ssim
from PIL import Image
from pathlib import Path
import matplotlib.pyplot as plt

# ==============================
# Dataset Class
# ==============================


class SketchToImageDataset(Dataset):
    def __init__(self, root, transform=None):
        self.root = root
        self.transform = transform
        self.image_paths = sorted(list(Path(root).glob("*.jpg")))

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, index):
        image = Image.open(self.image_paths[index]).convert("RGB")
        width, height = image.size
        sketch = image.crop((0, 0, width // 2, height))
        target = image.crop((width // 2, 0, width, height))
        if self.transform:
            sketch = self.transform(sketch)
            target = self.transform(target)
        return sketch, target


# ==============================
# Generator (U-Net Architecture)
# ==============================
class GeneratorUNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=3):
        super(GeneratorUNet, self).__init__()

        def down_block(in_feat, out_feat, normalize=True):
            layers = [nn.Conv2d(in_feat, out_feat, kernel_size=4,
                                stride=2, padding=1, bias=False)]
            if normalize:
                layers.append(nn.BatchNorm2d(out_feat))
            layers.append(nn.LeakyReLU(0.2))
            return nn.Sequential(*layers)

        def up_block(in_feat, out_feat, dropout=0.0):
            layers = [
                nn.ConvTranspose2d(
                    in_feat, out_feat, kernel_size=4, stride=2, padding=1, bias=False),
                nn.BatchNorm2d(out_feat),
                nn.ReLU(inplace=True)
            ]
            if dropout:
                layers.append(nn.Dropout(dropout))
            return nn.Sequential(*layers)

        self.down1 = down_block(in_channels, 64, normalize=False)
        self.down2 = down_block(64, 128)
        self.down3 = down_block(128, 256)
        self.down4 = down_block(256, 512)
        self.down5 = down_block(512, 512)
        self.down6 = down_block(512, 512)
        self.down7 = down_block(512, 512)
        self.down8 = down_block(512, 512, normalize=False)

        self.up1 = up_block(512, 512, dropout=0.5)
        self.up2 = up_block(1024, 512, dropout=0.5)
        self.up3 = up_block(1024, 512, dropout=0.5)
        self.up4 = up_block(1024, 512)
        self.up5 = up_block(1024, 256)
        self.up6 = up_block(512, 128)
        self.up7 = up_block(256, 64)
        self.up8 = nn.Sequential(
            nn.ConvTranspose2d(128, out_channels,
                               kernel_size=4, stride=2, padding=1),
            nn.Tanh()
        )

    def forward(self, x):
        d1 = self.down1(x)
        d2 = self.down2(d1)
        d3 = self.down3(d2)
        d4 = self.down4(d3)
        d5 = self.down5(d4)
        d6 = self.down6(d5)
        d7 = self.down7(d6)
        d8 = self.down8(d7)
        u1 = self.up1(d8)
        u2 = self.up2(torch.cat([u1, d7], 1))
        u3 = self.up3(torch.cat([u2, d6], 1))
        u4 = self.up4(torch.cat([u3, d5], 1))
        u5 = self.up5(torch.cat([u4, d4], 1))
        u6 = self.up6(torch.cat([u5, d3], 1))
        u7 = self.up7(torch.cat([u6, d2], 1))
        u8 = self.up8(torch.cat([u7, d1], 1))
        return u8


# ==============================
# Multi-Scale Discriminator (PatchGAN)
# ==============================
class Discriminator(nn.Module):
    def __init__(self, in_channels=6):
        super(Discriminator, self).__init__()

        def disc_block(in_feat, out_feat, normalize=True):
            layers = [nn.Conv2d(in_feat, out_feat,
                                kernel_size=4, stride=2, padding=1)]
            if normalize:
                layers.append(nn.BatchNorm2d(out_feat))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return nn.Sequential(*layers)

        self.model = nn.Sequential(
            disc_block(in_channels, 64, normalize=False),
            disc_block(64, 128),
            disc_block(128, 256),
            disc_block(256, 512),
            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)
        )

    def forward(self, x):
        return self.model(x)


class MultiScaleDiscriminator(nn.Module):
    def __init__(self, in_channels=6):
        super(MultiScaleDiscriminator, self).__init__()
        self.discriminator1 = Discriminator(in_channels)
        self.discriminator2 = Discriminator(in_channels)

    def forward(self, img_A, img_B):
        x = torch.cat([img_A, img_B], 1)
        out1 = self.discriminator1(x)
        out2 = self.discriminator2(
            nn.functional.interpolate(x, scale_factor=0.5))
        return out1, out2


# ==============================
# Perceptual Loss
# ==============================
class PerceptualLoss(nn.Module):
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        vgg = vgg19(pretrained=True).features
        self.layers = nn.Sequential(*list(vgg[:12])).eval()
        for param in self.layers.parameters():
            param.requires_grad = False

    def forward(self, fake_image, real_image):
        fake_features = self.layers(fake_image)
        real_features = self.layers(real_image)
        return nn.functional.mse_loss(fake_features, real_features)


# ==============================
# Training Setup
# ==============================
device = "cuda" if torch.cuda.is_available() else "cpu"

transform = transforms.Compose([
    transforms.Resize((286, 286)),
    transforms.RandomCrop((256, 256)),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2,
                           saturation=0.2, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

generator = GeneratorUNet().to(device)
discriminator = MultiScaleDiscriminator().to(device)
criterion_GAN = nn.BCEWithLogitsLoss()
criterion_pixelwise = nn.L1Loss()
perceptual_loss = PerceptualLoss().to(device)
lambda_pixel = 100

optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = optim.Adam(discriminator.parameters(),
                         lr=0.0002, betas=(0.5, 0.999))

scheduler_G = torch.optim.lr_scheduler.StepLR(
    optimizer_G, step_size=50, gamma=0.5)
scheduler_D = torch.optim.lr_scheduler.StepLR(
    optimizer_D, step_size=50, gamma=0.5)

scaler = torch.cuda.amp.GradScaler()


def train(dataloader, epochs=200):
    for epoch in range(epochs):
        for i, (sketch, real_image) in enumerate(dataloader):
            sketch, real_image = sketch.to(device), real_image.to(device)

            # Train Discriminator
            optimizer_D.zero_grad()
            with torch.cuda.amp.autocast():
                real_preds1, real_preds2 = discriminator(sketch, real_image)
                fake_image = generator(sketch)
                fake_preds1, fake_preds2 = discriminator(
                    sketch, fake_image.detach())
                loss_real = (criterion_GAN(real_preds1, torch.ones_like(real_preds1)) +
                             criterion_GAN(real_preds2, torch.ones_like(real_preds2))) / 2
                loss_fake = (criterion_GAN(fake_preds1, torch.zeros_like(fake_preds1)) +
                             criterion_GAN(fake_preds2, torch.zeros_like(fake_preds2))) / 2
                loss_D = (loss_real + loss_fake) / 2
            scaler.scale(loss_D).backward()
            scaler.step(optimizer_D)

            # Train Generator
            optimizer_G.zero_grad()
            with torch.cuda.amp.autocast():
                fake_preds1, fake_preds2 = discriminator(sketch, fake_image)
                loss_GAN = (criterion_GAN(fake_preds1, torch.ones_like(fake_preds1)) +
                            criterion_GAN(fake_preds2, torch.ones_like(fake_preds2))) / 2
                loss_pixel = criterion_pixelwise(fake_image, real_image)
                loss_perc = perceptual_loss(fake_image, real_image)
                loss_G = loss_GAN + lambda_pixel * loss_pixel + 0.1 * loss_perc
            scaler.scale(loss_G).backward()
            scaler.step(optimizer_G)
            scaler.update()

            if i % 100 == 0:
                print(f"Epoch [{epoch}/{epochs}] Batch [{i}/{len(dataloader)}]: "
                      f"Loss_D: {loss_D.item():.4f}, Loss_G: {loss_G.item():.4f}")

        scheduler_G.step()
        scheduler_D.step()

        torch.save(generator.state_dict(), f"generator_epoch_{epoch}.pth")
        torch.save(discriminator.state_dict(),
                   f"discriminator_epoch_{epoch}.pth")


# ==============================
# Visualization
# ==============================

def visualize(generator, dataloader, device="cuda"):
    """Visualize results from the generator using a batch of sketches from the dataloader."""
    generator.eval()
    with torch.no_grad():
        # Fetch a batch of sample sketches
        sample_sketch, sample_target = next(iter(dataloader))
        sample_sketch, sample_target = sample_sketch.to(
            device), sample_target.to(device)

        # Generate images from sketches
        generated_images = generator(sample_sketch).cpu()

        # Unnormalize images for visualization
        def unnormalize(img):
            img = img * 0.5 + 0.5  # Convert [-1, 1] to [0, 1]
            return img

        # Prepare a grid of sketches, generated images, and real images
        sample_sketch = unnormalize(sample_sketch.cpu())
        sample_target = unnormalize(sample_target.cpu())
        generated_images = unnormalize(generated_images)

        n_samples = min(5, sample_sketch.size(0))  # Show up to 5 samples
        fig, axes = plt.subplots(n_samples, 3, figsize=(15, 5 * n_samples))
        for i in range(n_samples):
            axes[i, 0].imshow(sample_sketch[i].permute(1, 2, 0))
            axes[i, 0].set_title("Sketch")
            axes[i, 0].axis("off")

            axes[i, 1].imshow(generated_images[i].permute(1, 2, 0))
            axes[i, 1].set_title("Generated Image")
            axes[i, 1].axis("off")

            axes[i, 2].imshow(sample_target[i].permute(1, 2, 0))
            axes[i, 2].set_title("Real Image")
            axes[i, 2].axis("off")

        plt.tight_layout()
        plt.show()

# Example usage:
# visualize(generator, test_dataloader, device)
